Introduction to Data Science

INTRODUCTION AND AIM
Motivation and Goal of the Investigation
Predicting the song's popularity is a fascinating challenge that bridges the domain of data science and creative industries.  In the modern music industry, several platforms like Spotify and Apple Music rely on data-driven decisions to solve different use cases, such as personalized recommendations, predicting the success of an unreleased song, and more. In this context understanding what contributes to the success of a song is crucial since it can help streaming platforms deliver a better user experience but also empowers the artists and producers to make informed decisions and help them which song to buy copyright for and release on their platform which can lead to certain profit margins for instance, In 2022, Spotify reported over 456 million users globally, emphasizing the importance of accurate music recommendations and popularity predictions for improving user retention and satisfaction.
The goal of the research is to predict the popularity of the songs available in the MusicOSet Dataset which is a multimodal dataset containing diverse features such as song and artist characteristics and hence using these features to train different machine learning algorithms to compare the algorithms and the dataset has lyrical dataset allowing the extraction of advanced features like sentiment polarity, and subjectivity. This allows for an analysis of how lyrical attributes impact the predictive models, providing a comprehensive understanding of the factors influencing song popularity.
Literature Review
The prediction of song popularity has been a compelling area of research due to its applications in music recommendation systems, marketing strategies, and understanding listener preferences. Several studies have utilized machine learning techniques to address this challenge. For example, the study "Predicting Song Popularity Using Spotify Web API" analyzed over 160,000 songs and employed models like Random Forest and XGBoost to predict popularity using features such as tempo, loudness, and genre, achieving high predictive accuracy [1]. Similarly, a Stanford University project titled "Song Popularity Prediction" applied regression models, neural networks, and support vector machines, emphasizing the importance of metadata and audio features in predicting song success [2]. These studies highlight the role of multimodal data in achieving accurate predictions, aligning with this research's focus on integrating diverse song, and artist characteristics.
Beyond structural and audio features, the role of lyrical content has gained attention for its impact on a song’s success. The paper "Quantitative Sentiment Analysis of Lyrics in Popular Music" analyzed Billboard Hot 100 songs over decades, revealing correlations between emotional content and popularity trends [3]. Another study, "Hit Song Prediction: Feature Engineering with Metadata and Lyrics," demonstrated the influence of sentiment polarity and repetition metrics on model performance, with Random Forest and Logistic Regression models outperforming others [4]. Furthermore, the paper "Beyond Beats: A Recipe to Song Popularity?" explored multimodal approaches combining lyrical, audio, and artist data, showing that integrating diverse features significantly enhances prediction accuracy [5]. This body of work supports the approach in this study, which incorporates lyrical features like sentiment polarity alongside traditional song attributes to analyze their collective impact on song popularity.
Research Aim
The aim of this study is to develop and evaluate predictive models for forecasting song popularity using multimodal data, including song features, artist attributes, and acoustic features. A baseline linear regression model will provide foundational insights, followed by the development and optimization of two non-linear machine learning models for a comparative analysis. Feature engineering will be performed to incorporate lyrical attributes, such as sentiment polarity and linguistic features, to assess their impact on predictive accuracy. The study aims to identify the most significant factors influencing song popularity by analyzing feature importance and evaluating the strengths and limitations of each modeling approach.
Research Questions
1.  What combination of song, artist, and acoustic features contributes most to the accurate prediction of song popularity for optimized machine learning models?
2.  How does the inclusion of lyrical features, such as sentiment polarity and other linguistic attributes, enhance the predictive accuracy and interpretability of machine learning models after optimization?
METHODOLOGY
Illustration of Methodology
Data Source and Gathering
The data that has been used is called “MusicOSet music dataset” [1]. This dataset has different musical elements consisting of Music data, Albums Data and Artist Data which also include the acoustic and lyrical data for the music makes it ideal for music data mining. Its rich metadata makes it quite useful for features engineering.
The data is available as CSV and SQL script that generates the data. This project uses SQL Script which was on MySQL Server to generate a relational database called ‘MusicOSet’. The data was then ingested into R environment using RMySQL library, using function like dbConnect() to connect to the database and dbSendQuery() for executing SQL queries. Once Ingested, the data was converted to data tables for high performance manipulation and for this we have used the library called data.table and then saved as RData files which can be used to ingest the data without querying it.
SQL was preferred over CSV files to efficiently handle the relational structure of the MusicOSet dataset, which spans multiple interrelated tables. SQL enabled selective data retrieval through optimized queries, reducing memory consumption and processing redundant data. Additionally, SQL provides a reproducible framework that is scalable that aligns with best practices in data science ensuring it’s adaptable for larger datasets in future.
Exploratory Data Analysis
Exploratory data analysis was done to find the pattern, trend, and anomaly of this dataset, showing the relationship among features with respect to the research questions. This section discusses insights related to the distribution, correlation, and other important aspects serving as the bedrock of feature selection and model training.

Firstly, we analyzed the distribution of popularity of the songs since it’s the target variable which we are going to predict. This distribution in Figure 1 revealed significant variability across the dataset, with a large concentration of songs in the lower popularity ranges. This skew in the target variable emphasized the importance of using evaluation metrics, such as RMSE to capture both central and extreme predictions accurately. Additionally, because it’s a continuous variable it justifies the use of regression models.


We also checked the distribution of types of songs and if they are explicit or not and plotted pie charts. Figure 2 showing that most songs are non-explicit, furthermore Figure 3 highlights that solo songs significantly outnumber collaborations. These two features are included in the analysis since we want to understand how every feature affect the regression modelling.
 

One thing we noticed when it came to artist features, the total number of artist followers given in Figure 4 has shown growth over the years illustrating a steep increase in recent decade. This trend underscores the importance of including artist popularity metrics, such as total followers, as key predictors in the modeling process. [add artist types]




 

The distributions of acousticness, danceability, energy, and valence in Figure 5 reveal their variability and potential influence on song popularity. For instance, danceability and energy exhibit balanced distributions, while acousticness skews lower. These features are often emphasized in music literature for their role in audience engagement, supporting their inclusion in predictive modeling.  (https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0275228&utm_source=chatgpt.com)
 

Figure 7 illustrates the correlation heatmap of non-lyrical features, including acoustic and artist-related attributes. Strong correlations are observed between popularity and features like total followers, average artist popularity, and acoustic features such as danceability and energy, emphasizing their influence on a song's success. Explicit content shows weaker correlations, suggesting its impact is context dependent. These insights informed the prioritization of significant features during model training.

This EDA provided critical insights into the dataset’s structure, distributions, and relationships. Examining key attributes such as popularity and explicit content revealed some pattern that are crucial for data processing and initially framing problem type either classification or regression and helped in selecting the evaluation metrics like RMSE and R-Squared which measure prediction error and model fit. One more that came to notice is that label encoding also must take place since, song type and explicit/non-explicit are categorical variables and must change for interpretability by the regression model. Furthermore, Although the popularity distribution is a bit skewed, this reflects the real-world nature of the data, where only a subset of songs achieves high popularity, so the outliers were not removed.

Finally, based on the correlation heatmap, it showed that some features like tempo and key had a very weak correlation with popularity, but they were not dropped because they could still contribute valuable information through non-linear relationships.
Data preprocessing and Cleaning
The aim is to predict the song popularity, so we need distinct Song IDs to work with, so firstly we dropped the rows that doesn’t have any Song ID, fortunately there was only one datapoint with an empty string in Song ID, so we dropped it.
Then comes the process of adding songs features, artist characteristics, acoustic features, and then finally the lyrical features that we will engineer. Firstly, we add the songs features like the latest year_end_score and year for the songs from df_pop_songs. The latest year_end_score is taken to capture the most UpToDate status of the song sine the there were several year_end_score given for different years and for the songs that doesn’t have the year_end_score available we impute the values with average year_end_score with the songs have the same popularity because this preserved the relationship between popularity and year_end_score furthermore, the missing values for years were replaced with median which robustly handles outliers. For features like explicit and song type which are categorical, we do the label encoding for them so it’s interpretable by the machine learning model afterwards we joined the acoustic features to the dataset such as tempo, danceability and energy.
Then comes the artist features, Firstly we clean up the artist dataset and make it more consistent for example the artist type has a few null which are given as a string as ‘-’ so these are given a new category called ‘not given’, this is done since we can safely assume that an artist has to be of some category and we don’t have to drop it furthermore, we have to change one the artist time ‘band’ which was inconsistent. Then, the follower’s data type as change to numeric since it can only be a numeric character but while doing that, it was realized there were inconsistent strings as ‘None’ which is nothing just 0 followers so that is also made consistent.
Then comes the aggregation and addition of these artist features, why aggregation? There’s one thing to note that one song can have more than one artist, and since the artist data is given on an artist level to join these features to song, we need to aggregate whenever there’s more than one artist involved in the song making.
We added 6 features after aggregation:
1.  num_artist: Number of artists that collaborated. This captures the scale of collaboration, which can influence a song’s appeal and reach.
2.  total_artist_followers: Total number of followers of the contributing artists. A higher follower count indicates a larger fanbase, potentially contributing to a song’s popularity.
3.  avg_artist_popularity: Average popularity score of the contributing artists. Popular artists often bring a significant boost to a song’s visibility and success.
4.  unique_artist_main_genre: Number of unique main genres of the contributing artists. Diverse genres in collaboration can attract a broader audience and impact popularity.
5.  unique_artist_type: Number of unique artist types involved. Including varied artist types (e.g., bands, solo singers) enriches the song's creative dimensions.
6.  avg_artist_year_end_score: Average latest year-end score of the artists collaborating in the song. Using the latest score ensures the most up-to-date representation of an artist’s impact on song success.
For the corner cases where there was no year_end_score given for the artist, we imputed those values with the mean of the average year_end_score of the artist. We used mean here to reflect the overall trend in the artist score and since it also a representative value.
The lyrical features are not available in the default data, so we engineer them since we have a research question for that. This process is given in the next section “Feature Engineering”.
After all the data preprocessing, cleaning and features engineering is done, the data is split into 70:30 ratio which adhere to the standard data science practice since this ensure that sufficient data is there for model training meanwhile enough is present for testing the accuracy of the models. One last thing to mention here is that the song popularity is a value from 0-100, so we scale the popularity by dividing it by 100 so the values are between 0-1 since it will help the models to achieve faster convergence and improved accuracy [add reference].
Lyrical Feature Engineering
This features engineering is carried out to derive meaningful variables from the dataset specifically from the lyrical text data which aligns with our research question.
In the previous section we have done some features engineering as well where we aggregated the artist features to include in the analysis, but here we have use only the lyrics text data.
Firstly, we have cleaned the lyrical data that is given in the data table df_lyrics. The cleaning for this text data is done in steps given below.
7.  Standardizing Text:  To standardize the text, we have removed the special characters and punctuation. This is done to have uniform text representation and also it’s a common NLP practice.
8.  Handling the Non-Lyrical Entities (Corner Cases): While doing the analysis I came across some corner cases where the the lyrics are given as “”, “ssss”, and “instrumental”/”instrumental tracks” which are invalid or non-lyrical.
For these cases all the values for the derived features were set to neutral default values since we can’t identify features when we don’t have text for instance the sentiment of the lyrics can’t be identified so they were given a neutral values. This prevents bias and make sure these songs doesn’t create skew features distribution.
9.  Stop words handling:  Stop words were retained for some features to get the original word count but removed for others like lexical diversity. Retention of stop words give a more accurate total word count when required. so we end up having two columns with lyrics with and without stop words so they can be used for the respective features.
After completing the cleaning of the text data, relevant features were engineered, carefully selected based on their potential utility and alignment with the research objectives. These features are given below:
10. Sentiment Polarity: The sentiment polarity measure the overall emotional tone of the lyrics, this features is important because emotionally charged lyrics may resonate with the listener and might affect it’s popularity. These are calculated using Syuzhet package, and then they are normalized using the word count (with stop words). Normalization ensures fair comparison across songs of varying lengths by standardizing the emotional tone per word.
11. Objectivity: This features tells use the neutrality of the lyrics since we know the neutral lyrics can target a broader audience but subjective lyrics can evoke stronger emotions which can lead to a change in the popularity. Objectivity is also normalized with the word count (including stop words)
12. Word Count: This is the total number of words including the stop words and this helps in capturing the overall verbosity of the lyrics which reflects storytelling complexity and lyrical density.
13. Lexical Diversity: Lexical diversity captures the linguistic variety. High diversity can make a song unique and engaging. Here, we use the Type-token Ratio, which is the ratio of the number of unique words to the number of total words. Here the stop words are included as well.
14. Average Word Length: This features captures the mean number of characters per word (including the stop words). Longer words may indicate sophistication in lyrical composition.
15. Repetition Ratio: Finally, repetition ration captures the proportion of the repeated words in the lyrics. The rationale behind it is that the repetition enhances the catchiness so called memorability of the songs which can directly have a correlation with popularity. for instance “bum bum tum tum by Mc Fioti”, has been a big hit but with repeated lyrics.
After the feature are engineered, we join the main lyrical features with the other features. One thing to note here, there are about 3.6% of the songs that doesn’t have any entry in the lyrical database. Since these datapoints lacked corresponding data in the lyrical dataset, it made it impossible to derive meaning features from them and imputing them would create artificial data and could distort the analysis. So we dropped them as the standard data preprocessing practice of dropping entries when the proportion of missing data is less than 5% [2]. The example of pre and post processing lyrical data is given below.

Model Training and Hyperparameter Tuning
After the preprocessing is done, the comes the training and optimization of the predictive models for predicting the song popularity which is a regression problem. The models that are used in this study are Linear Regression (baseline), Random Forest and XGBoost. Each model was trained on a dataset containing song features, artist attributes, and engineered lyrical features. These models are tested before and after the addition of the lyrical features to see how it affects the predictive power of the model, furthermore, the hyperparameter tuning is also done for the Random Forest and XGBoost Regressors using gridSearch method with cross validation. A 5-fold cross-validation technique was used to evaluate model performance across multiple subsets of the training data, reducing the risk of overfitting and Grid search was chosen as the hyperparameter tuning approach to systematically test the combinations of hyperparameters, this also ensures that the best parameters of the models are saved. This process of training with and without lyrical feature followed by hyperparameter tuning improve the accuracy and interpretability of the models which is inconjunction with both of the research questions. Afterwards the models with the best parameters are finalized and test using the testing data (30%) and then evaluated using the evaluation metrics and other plots. The details of the models are given below:
1.  Linear Regression: Linear Regression serves as the baseline model to evaluate the effectiveness of more complex machine learning algorithms. It’s chosen as a baseline model to establish a foundation benchmark since it’s a very simple modelling algorithm. It is implemented using the ‘lm’ function in R, furthermore, the model is trained without tuning since linear regression lacks hyperparameter options.
2.  Random Forest: Random Forest is a bit more advanced machine learning algorithm and chose for its robustness in handling high-dimensional data and its ability to identify feature importance. It is implemented using a package called caret with ranger implementation. Then the grid search strategy is with 5-fold cross validation was used to optimize 3 parameters that are given below.
Parameter Name  Definition  Values after Hyperparameter tuning
mtry    number of features for splits   6
splitrule   criteria for node splitting (variance used generally for regression)    ‘variance’
min.node.try    minimum size of terminal nodes  1

3.  XGBoost: XGBoost is also an advanced machine learning algorithm which is chosen because of its efficiency in handling structured data and imbalanced datasets and also it has its own functions to calculate the features importance. It is also implemented using a package called caret with ranger implementation. Then the grid search strategy is with 5-fold cross validation used to optimize 7 given below.  [https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/]
Parameter Name  Definition  Values after Hyperparameter tuning (without lyrics) Values after Hyperparameter tuning (with lyrics)
nrounds number of boosting rounds   150 150
max_depth   maximum tree depth  6   6
eta loss for split  0.1 0.05
min_child_weight    Minimum sum of weights for child nodes (Regularization) 3   3
Evaluation Metrics
Finally, to effectively evaluate the performance of the predictive models, we are using three metrics that are widely accepted for regression problems that are given below:
•   Root Mean Squared Error (RMSE):  RMSE measure the average magnitude of the prediction error by emphasizing large errors. This is particularly useful for identifying models that predict the high and low values of the popularity. It ensures the model’s robustness for extreme popularity scores.Its’s used in a similar study "Predicting Song Popularity Using Machine Learning Algorithms" by Kakkad et al., where regression models were evaluated for song popularity prediction using this metric [3].

•   R-Squared (R²):  R² quantifies the proportion of variance in the target variable (popularity) that the model explains so higher the R-Squared value, the better the model explains the variance. It is particularly beneficial for interpreting the effectiveness of feature selection and engineering as highlighted in "Predicting Song Popularity" by Stanford researchers, R² is a critical metric for assessing model performance in the domain of song popularity prediction [5].
To further assess model performance and add a visual component to it, A residual analysis is done, and the distribution is plotted. The residual here is just the difference between the actual and predicted values. This is useful to understand the error centering, where a good model will have the residuals centered around zero indicating unbiased prediction across the popularity scores and the error spread, this gives an idea about the variability in the prediction since the narrower spread indicates higher accuracy and consistency.
These density plots of residuals were created for each model to compare performance in conjunction with the RMSE and R-Squared metrics.
ANSWERING THE RESEARCH QUESTIONS
What combination of song, artist, and acoustic features contributes most to the accurate prediction of song popularity for optimized machine learning models?
To address this question, song, artist, and acoustic features are combined and preprocessed into a unified dataset. Features that can be relevant are selected, including energy, danceability, and artist genre. Afterward, three machine learning models were trained on the 70-30 train-test split with optimized hyperparameters using grid search with cross-validation: Linear Regression, Random Forest, and XGBoost. Feature importance metrics, such as the IncNodePurity (https://www.geeksforgeeks.org/feature-importance-with-random-forests/?utm_source=chatgpt.com) and gain (https://mljourney.com/xgboost-feature-importance-comprehensive-guide/?utm_source=chatgpt.com) which measure a feature's contribution to reducing variance or improving the model's objective function, were examined to pinpoint which features best explain the popularity of a song. This structured approach would ensure that the research ascertains essential predictors while assessing the effectiveness of refined models.
By examining feature importance values and residual plots, the analysis uncovers relationships between feature combinations and prediction accuracy. The results quantitatively demonstrate which attributes significantly influence popularity predictions and how various models interpret these features, answering the research question while aligning with the study’s broader aim of uncovering key drivers of song popularity.
How does the inclusion of lyrical features, such as sentiment polarity and other linguistic attributes, enhance the predictive accuracy and interpretability of machine learning models after optimization?
This question is addressed by adding lyrical features of the songs, such as sentiment polarity, objectivity, and lexical diversity to the dataset. Both lyrical and nonlyrical datasets are used for the training of models to identify the effect of lyrical features on the predictive performance based on metrics such as RMSE and R². Residual plots and feature importance metrics highlight how these features impact model predictions and interpretability. By comparing results, the study evaluates whether lyrical features improve prediction accuracy and provide deeper insights into the creative and emotional aspects of songs.
Lyrical features offer a further dimension of understanding of the drivers of popularity. Indeed, findings indicate that these increase model performance and interpretability, effectively answering the research question, while aligning with the objective of improving the accuracy through integrations of the feature.

RESULTS AND DISCUSSION
This section presents the outcomes of the analysis, integrating insights gained from exploratory data analysis (EDA), model training, and evaluation. The results aim to address the research questions by highlighting the effectiveness of the models in predicting song popularity, understanding the role of different features—including lyrical attributes—and comparing their contributions.
Initial observations from EDA were instrumental in making key methodological decisions, such as selecting preprocessing techniques and understanding the data's structure. For instance, analyzing the distribution of song popularity [figure in EDA] confirmed that regression modeling was the appropriate choice and informed the selection of evaluation metrics like R² and RMSE. Additionally, EDA revealed that categorical variables like explicit type and song type required label encoding for compatibility with the regression models. Correlation analysis further highlighted the relationships between features and song popularity, guiding feature selection. Post-training analyses, including feature importance and residual patterns, provided deeper insights into model behavior. The discussion that follows focuses on the models' performance, feature contributions, and the implications of including lyrical features, aligning these findings with the research objectives.

MODEL TRAINING RESULTS
Model training was done after all the data preprocessing is done and data is split. The data is split to a ration of 70:30 for training and testing respectively which is an accepted data science standard. The model training is done with data with lyrical features and without lyrical features aligning with the RQs to understand what type of attributes, have the most impact on song popularity and furthermore the lyrical features impact.
For evaluation we have used 3 metrics RMSE and R-Squared which came about to be based on the EDA. The results are given below.



Model Name  Without Lyrical Features    With Lyrical Features
    RMSE    R-Squared   RMSE    R-Squared
Linear Regression (Baseline)    14.081  0.589   14.072  0.588
Random Forest   12.697  0.667   12.894  0.658
XGBoost 12.822  0.659   12.711  0.664

The results demonstrate varying levels of performance across the models, reflecting the influence of feature selection and model complexity. The baseline linear regression model showed moderate predictive capability, achieving an R-Squared of 0.589 and an RMSE of 14.081 without lyrical features. The inclusion of lyrical features resulted in only a negligible improvement (0.06% reduction in RMSE), highlighting that the linear relationship between song popularity and lyrical features is either weak or dominated by other factors. This suggests that linear models may not effectively capture the nuances introduced by lyrical attributes.
The random forest model, leveraging its ability to capture non-linear relationships, outperformed the baseline with an R-Squared of 0.667 and RMSE of 12.697. However, when lyrical features were included, performance declined slightly, with RMSE increasing by 1.55% and R-Squared dropping to 0.658 indicating a decrease in the model's ability to explain variance in the data. This indicates that while random forest excels at handling complex interactions, lyrical features may contribute redundancy or noise rather than meaningful predictive value.
XGBoost, known for its robustness and efficiency in handling structured data, performed similarly to random forest with an R-Squared of 0.659 and RMSE of 12.822 without lyrical features. The inclusion of lyrical features yielded minimal change, with R-Squared slightly increasing to 0.664. These results suggest that the non-linear relationships captured by XGBoost are not significantly enhanced by lyrical features, potentially due to the limited or noisy nature of these attributes.
FEATURE IMPORTANCES ANALYSES

Fearure Importance Before Adding Lyrical Features   Feature Importance after Adding Lyrical Features

     
     
     
The feature importance analysis for Linear Regression, Random Forest, and XGBoost highlights the varying influence of different attributes in predicting song popularity. In the Linear Regression model, objectivity, explicit content, and speechiness emerged as key contributors, while lyrical features like lexical diversity and sentiment polarity showed minimal impact, aligning with their limited effect on improving the model's performance. Random Forest and XGBoost, known for capturing complex patterns, identified average artist popularity, total artist followers, and year as the most significant features. These results emphasize the critical role of artist-related metrics, while lyrical features contributed marginally and, in some cases, introduced noise.
Across all models, artist and acoustic features consistently dominated in importance, reflecting their direct impact on popularity prediction. Lyrical features, while theoretically relevant, showed limited practical influence on predictive accuracy. This reinforces the idea that predictive modeling for song popularity benefits from prioritizing artist and acoustic attributes, with lyrical features playing a supplementary role. These findings align with earlier observations of negligible performance improvements when lyrical features were included.
DISCUSSION
Random Forest demonstrated the best overall performance, achieving the lowest RMSE (12.697) and highest R-Squared (0.667) without lyrical features, outperforming both the baseline and XGBoost model, furthermore, After the addition of the lyrical features, it didn’t improve the predictive accuracy instead it hindered the performance of Random In contrast, XGBoost showed a marginal improvement in RMSE with the inclusion of lyrical features, though the gain was negligible. Hence it can be inferred from this, that lyrical features may introduce redundancy or noise, contributing little to enhancing the model’s predictive power.
These findings underscore the dominance of artist and acoustic features in determining song popularity while questioning the practical utility of the lyrical attributes in predictive modelling in this use case.
ANSWERING THE RESEARCH QUESTION
•   What combination of song, artist, and acoustic features contributes most to the accurate prediction of song popularity for optimized machine learning models?
The analysis revealed that artist-related and acoustic features, particularly average artist popularity, total artist followers, and year-end score, were consistently among the most influential predictors of song popularity across all models. Random Forest and XGBoost highlighted these features as highly impactful, confirming their importance in determining popularity. Additionally, acoustic attributes such as energy and instrumentalness also demonstrated moderate relevance, reflecting the significant role of musical characteristics in engaging listeners. Interestingly, some features, like tempo and key, had minimal influence, suggesting they may contribute weakly to popularity or interact non-linearly with other features. 
•   How does the inclusion of lyrical features, such as sentiment polarity and other linguistic attributes, enhance the predictive accuracy and interpretability of machine learning models after optimization?
Contrary to initial expectations, the inclusion of lyrical features, such as sentiment polarity, lexical diversity, and repetition ratio, did not substantially improve model performance. Linear regression showed negligible improvement (0.06% in RMSE), while non-linear models like Random Forest experienced slight declines in accuracy, with RMSE increasing by 1.55% and 0.06%, while XGBoost showed a slight improvement, with RMSE decreasing by 0.86% respectively. These results suggest that while lyrical features provide interpretive value regarding emotional and linguistic composition, but they may introduce redundancy or fail to complement the dominant predictive power of artist and acoustic features. This outcome highlights the nuanced role of lyrical features, indicating their limited predictive performance.
RELATION TO EXISTING RESEARCH
The findings align with existing literature in various ways. Firstly, the significant contributions of features like total artist followers and average artist popularity are consistent with the work of Sharma and Singh (2021), who identified artist-centric features as strong predictors of song success. Their research highlights that an artist’s established popularity and reach play a crucial role in determining the likelihood of a song becoming a hit, the relatively weak contribution of lyrical features to predictive accuracy supports the findings by Anderson et al. (2020), who noted that acoustic and metadata-related features tend to outperform lyrical attributes in predicting song popularity. This study similarly observed that lyrical features, while valuable for interpretability, introduced marginal noise in non-linear models like Random Forest and XGBoost, slightly reducing their performance.
Furthermore, the importance of acoustic features such as danceability, loudness, and energy echoes the observations in previous research (e.g., Kim et al., 2020), which underscored the emotional and sensory appeal of songs as central to their popularity. These findings the value of incorporating diverse feature categories while suggesting that some may hold higher predictive utility than others. However, the limited contribution of lyrical features observed in this study contrasts with some prior research suggesting that lyrical content can influence song popularity in the work of Yang Gao and John Harden (2016). This study focused on the Billboard Hot 100 songs indicated that lyrical features, such as sentiment and complexity, could impact a song's chart performance.
R CODE AND GITHUB PAGES



CONCLUSION

The project aimed to develop and evaluate predictive models for song popularity using multimodal data consisting of song, artist and acoustics features to see are most influential for predicting song popularity. furthermore, lyrical features were also engineered to see their impact on the predictive accuracy of the model. Three machine learning models were trained: linear regression that acted like the baseline model and then 2 more non-linear models, Random Forest and XGBoost. Exploratory data analysis guided key methodological decisions, followed by data preprocessing, model optimization, and evaluation of the models.

The results showed that non-linear models outperformed the baseline, but lyrical features had limited impact on the performance of the models highlighting the nuanced interplay between features and prediction models. This study offers valuable insights into the factors contributing to song popularity and the effectiveness of different predictive approaches.
KEY FINDINGS
1.  Random Forest outperformed both the baseline linear regression and XGBoost models, achieving the lowest RMSE (12.697) and highest R-Squared (0.667) without lyrical features. (Section: Model Training Results)
2.  Lyrical features had negligible or negative effects on the performance of non-linear models, potentially introducing noise due to their weaker or redundant correlations with song popularity. (Section: Model Training Results)
3.  Acoustic features like danceability and energy emerged as significant predictors of song popularity, as identified through feature importance analysis, while other features such as tempo and key were less impactful. (Section: Feature Importance and Comparison)
4.  Artistic features, particularly average artist popularity and total followers, were significant predictors of song popularity, especially for non-linear models. (Section: Feature Importance and Comparison)
5.  The success of non-linear models over the baseline suggests the presence of complex interactions among features, underscoring the value of models capable of capturing such relationships. (Section: Model Training Results)
LIMITATIONS, ASSUMPTIONS AND WEAKNESSES
LIMITATIONS
-   The dataset was limited to around 20k songs, which may not fully represent the diversity of global music trends and could introduce biases depending on its source. A larger dataset could capture more nuanced trends.
-   Due to the limited dataset size, advanced deep learning models (e.g., transformer-based models) using embeddings could not be trained, which might better capture the nuances of lyrical sentiment and patterns.
-   Computational constraints restricted extensive hyperparameter tuning, limiting the exploration of the full potential of non-linear models like Random Forest and XGBoost.
-   Additional features could be extracted from lyrical text data (e.g., thematic clustering or narrative structures), which might provide deeper insights into the relationship between lyrics and popularity.
ASSUMPTIONS
-   Scaling popularity scores between 0 and 1 was assumed to improve model interpretability and stability, especially for algorithms like XGBoost and Random Forest, which are sensitive to feature magnitude.
-   Acoustic, artist, and lyrical features were assumed to be sufficient to capture the key trends influencing song popularity, as they represent core aspects of music analysis.
WEAKNESS
-   Basic lyrical features, such as sentiment polarity, might oversimplify the complex and subjective impact of lyrics on listeners, missing deeper thematic or contextual elements.
-   The limited dataset increases the risk of overfitting for non-linear models despite applying hyperparameter tuning, potentially reducing the generalizability of the results.
-   The analysis did not consider genre-specific trends, which may significantly affect popularity, thereby limiting the applicability of findings across diverse music genres.
FUTURE WORK
Incorporating large datasets will include a more diverse range of songs across genres and time periods and could also improve the generalizing ability of the models and furthermore, with larger dataset better models can be trained like a DNN. When come to feature engineering, more advanced lyrical features can be extracted such as thematic clustering and more which might capture the trends of the lyrics better in conjunction with larger data and on top of that adding genre specific features can also be added and their impact could provide better insights as different genres exbibit distinct trends in song popularity.

Real time streaming data could also be leveraged coming from Spotify or YouTube (Listening platforms) which can improve the practicality of the research. Integrating user behavior data, including listener preferences or streaming habits, may also offer a more comprehensive understanding of the factors influencing song popularity, paving the way for more accurate and actionable predictions.

REFERENCES



